{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been looking over our strategy to enhance the loan approval process at MoneyLink Bank, and it's clear we need a solid plan grounded in data. Our customers are at the heart of what we do, and streamlining their experience is key. With this in mind, I'm assigning you a crucial task. You'll be developing four distinct Machine Learning models to help us pinpoint the leads most likely to gain loan approval. Our main goal is to make accurate predictions, but we also have to be efficient in terms of time, computational resources, and storage.\n",
    "\n",
    "You can start by opening the provided Jupyter notebook and running the first line of code to import the data. Then, you need to train for different machine learning models - logistic regression, decision tree, XGBoost and a Support Vector Machine (SVM) model. For each of the four models, make a note of the prediction accuracy, training and prediction time, and the model's storage usage. This data will be essential in your final report, where you'll compare these metrics for each model. I want a deep dive into which model best fits our needs, taking into account both its predictive power and its operational feasibility. Your insight will shape our bank's future strategies, so let's make sure we nail this. All the best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Data and Conduct Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data for analysis by performing the following actions:\n",
    "\n",
    "1. Differentiate the dataset into predictor variables and the target variable\n",
    "2. Handle the non-numerical attributes by creating dummy variables\n",
    "3. Implement a 60:40 train-test split using a random state of 42\n",
    "\n",
    "Load Data: \n",
    "Import the dataset and separate it into predictor variables (X) and the target variable (y).\n",
    "\n",
    "Handle Non-Numerical Attributes: \n",
    "Convert categorical variables into dummy/indicator variables using pd.get_dummies().\n",
    "\n",
    "Split Data: \n",
    "Split the data into training (60%) and testing (40%) sets with a fixed random state for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('../../Data/LeadScoring/loan_lead_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#3. Separate the predictor variables from the target variable\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoan Approved\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoan Approved\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#4. Transform non-numerical attributes using the pd.get_dummies method\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#3. Separate the predictor variables from the target variable\n",
    "\n",
    "X = data.drop('Loan Approved', axis=1) \n",
    "y = data['Loan Approved']\n",
    "\n",
    "#4. Transform non-numerical attributes using the pd.get_dummies method\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "#5. Divide the dataset into training and testing subsets using the train_test_split function, \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Address Class Imbalance Using Visualization and SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the class counts in the target variable and visualize the potential imbalance using a bar chart. If the visual confirms an imbalance in the target variable of the training set, utilize the Synthetic Minority Over-sampling Technique (SMOTE) to equalize the class distribution. Confirm class balance in the new data using another bar chart visualization.\n",
    "\n",
    "Visualize Class Distribution: Plot the distribution of the target variable before applying SMOTE.\n",
    "\n",
    "Apply SMOTE: Use Synthetic Minority Over-sampling Technique (SMOTE) to handle class imbalance in the training set.\n",
    "\n",
    "Visualize Class Distribution After SMOTE: Plot the distribution of the target variable after applying SMOTE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Import necessary libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#2. Plot a bar chart for y_train to show class counts.\n",
    "y_train.value_counts().plot(kind='bar', title='Class Distribution before SMOTE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Use SMOTE to balance out the dataset.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "#4. Visualize the balanced classes with another bar chart for verification\n",
    "y_train_smote.value_counts().plot(kind='bar', title='Class Distribution after SMOTE')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a Logistic Regression Model and Calculate Training Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Understanding Logistic Regression in Layman Terms\n",
    "\n",
    "Logistic regression is a statistical method used for predicting the outcome of a binary variable. In simple terms, it helps us decide between two possible outcomes based on some input information. Here's a straightforward explanation using a real-world example:\n",
    "\n",
    "#### Example Scenario: Predicting Loan Approval\n",
    "\n",
    "Imagine you work at a bank and you want to predict whether a loan application will be approved or not. Each application has various pieces of information, such as the applicant's income, credit score, and employment status.\n",
    "\n",
    "1. **Input Information:**\n",
    "   - Income\n",
    "   - Credit Score\n",
    "   - Employment Status\n",
    "\n",
    "2. **Possible Outcomes:**\n",
    "   - Approved (Yes)\n",
    "   - Not Approved (No)\n",
    "\n",
    "#### How Logistic Regression Works\n",
    "\n",
    "1. **Gather Data:**\n",
    "   - You collect data on past loan applications, noting which ones were approved and which ones were not, along with the details of each application (income, credit score, etc.).\n",
    "\n",
    "2. **Fit a Model:**\n",
    "   - Logistic regression uses this past data to understand the relationship between the input information (income, credit score, etc.) and the outcome (approved or not approved). It tries to find a mathematical equation that can predict the outcome based on the input information.\n",
    "\n",
    "3. **Predict Probability:**\n",
    "   - When a new loan application comes in, the logistic regression model uses the mathematical equation to calculate the probability of the loan being approved. This probability is a number between 0 and 1. \n",
    "     - A probability closer to 1 indicates a higher likelihood of approval.\n",
    "     - A probability closer to 0 indicates a higher likelihood of rejection.\n",
    "\n",
    "4. **Make a Decision:**\n",
    "   - Based on the calculated probability, you can set a threshold to make a decision. For instance, if the probability is above 0.5, you might decide to approve the loan. If it's below 0.5, you might decide to reject it.\n",
    "\n",
    "#### Visualizing the Concept\n",
    "\n",
    "- Imagine plotting the past loan applications on a graph where the x-axis represents one of the input factors (e.g., credit score) and the y-axis represents the probability of loan approval.\n",
    "- Logistic regression draws a curve (called a logistic curve) that best fits the data. This curve helps us understand how the input factors influence the probability of loan approval.\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- **Binary Outcome:** Logistic regression is used when the outcome can only be one of two possible values (e.g., approved or not approved).\n",
    "- **Probability:** It predicts the probability of the outcome.\n",
    "- **Threshold:** A decision threshold is used to classify the outcome based on the predicted probability.\n",
    "\n",
    "#### Simplified Formula\n",
    "\n",
    "In a very simplified way, the logistic regression model looks like this:\n",
    "\n",
    "\\[ \\text{Probability of Approval} = \\frac{1}{1 + e^{-(a + b \\cdot \\text{Income} + c \\cdot \\text{Credit Score} + d \\cdot \\text{Employment Status})}} \\]\n",
    "\n",
    "Where:\n",
    "- \\(a, b, c, d\\) are the coefficients that the model learns from the data.\n",
    "- \\(e\\) is the base of the natural logarithm.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "Logistic regression helps us make decisions between two possible outcomes by predicting the probability of each outcome based on input information. It’s like a smart assistant that, given the details of a loan application, tells you how likely it is that the loan will be approved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions in Logistic Regression\n",
    "\n",
    "Logistic regression, like all statistical models, is built on a set of assumptions. Here are the key assumptions:\n",
    "\n",
    "1. **Binary Outcome:**\n",
    "   - The dependent variable should be binary, meaning it has two possible outcomes (e.g., yes/no, success/failure).\n",
    "\n",
    "2. **Independence of Observations:**\n",
    "   - The observations should be independent of each other. Each data point should be unrelated to others.\n",
    "\n",
    "3. **No Multicollinearity:**\n",
    "   - The independent variables (predictors) should not be too highly correlated with each other. Multicollinearity can cause issues with the estimation of coefficients.\n",
    "\n",
    "4. **Linearity of Logit:**\n",
    "   - The relationship between the independent variables and the log odds of the dependent variable should be linear. This means that the log(odds) of the outcome can be modeled as a linear combination of the predictor variables.\n",
    "\n",
    "5. **Large Sample Size:**\n",
    "   - Logistic regression requires a sufficiently large sample size to produce reliable estimates. The rule of thumb is to have at least 10 events per predictor variable.\n",
    "\n",
    "### Odds and Log-Odds\n",
    "\n",
    "**Odds:**\n",
    "\n",
    "Odds are a way of expressing the likelihood of an event happening compared to it not happening. They are calculated as the ratio of the probability of the event occurring to the probability of it not occurring.\n",
    "\n",
    "\\[ \\text{Odds} = \\frac{\\text{Probability of Event}}{\\text{Probability of No Event}} \\]\n",
    "\n",
    "For example, if the probability of getting approved for a loan is 0.75 (75%), the odds of approval are:\n",
    "\n",
    "\\[ \\text{Odds} = \\frac{0.75}{1 - 0.75} = \\frac{0.75}{0.25} = 3 \\]\n",
    "\n",
    "This means that the odds of getting approved are 3 to 1.\n",
    "\n",
    "**Log-Odds:**\n",
    "\n",
    "Log-odds, or the logarithm of the odds, is used in logistic regression to model the relationship between the predictors and the outcome. The logistic function converts log-odds back to a probability.\n",
    "\n",
    "\\[ \\text{Log-Odds} = \\log\\left(\\frac{\\text{Probability of Event}}{\\text{Probability of No Event}}\\right) \\]\n",
    "\n",
    "Logistic regression models the log-odds as a linear combination of the predictor variables.\n",
    "\n",
    "### Interpretation in Logistic Regression\n",
    "\n",
    "1. **Coefficients (Log-Odds):**\n",
    "   - The coefficients in a logistic regression model represent the change in the log-odds of the outcome for a one-unit change in the predictor variable, holding other variables constant.\n",
    "\n",
    "2. **Odds Ratios:**\n",
    "   - The exponential of a coefficient gives the odds ratio (OR) for a one-unit increase in the predictor variable. The odds ratio tells us how much the odds of the outcome increase (OR > 1) or decrease (OR < 1) with a one-unit increase in the predictor.\n",
    "\n",
    "\\[ \\text{Odds Ratio} = e^{\\text{Coefficient}} \\]\n",
    "\n",
    "3. **Probability:**\n",
    "   - To convert the log-odds back to a probability, we use the logistic function:\n",
    "\n",
    "\\[ \\text{Probability} = \\frac{1}{1 + e^{-\\text{Log-Odds}}} \\]\n",
    "\n",
    "### Example Interpretation\n",
    "\n",
    "In the context of the logistic regression example provided, the predictor variables (also known as independent variables or features) are the factors used to predict the outcome (dependent variable). \n",
    "\n",
    "### Predictor Variables\n",
    "\n",
    "Here, the predictor variables are:\n",
    "\n",
    "1. **Income**: This represents the applicant's income.\n",
    "2. **Credit Score**: This represents the applicant's credit score.\n",
    "\n",
    "### Dependent Variable\n",
    "\n",
    "The outcome (dependent variable) that we are trying to predict is:\n",
    "\n",
    "- **Loan Approval**: This is a binary variable indicating whether a loan application is approved (Yes) or not approved (No).\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Suppose we have a logistic regression model for predicting loan approval with the following coefficients:\n",
    "\n",
    "\\[ \\text{Log-Odds} = -2 + 0.05 \\cdot \\text{Income} + 0.1 \\cdot \\text{Credit Score} \\]\n",
    "\n",
    "- **Intercept (-2):** When both income and credit score are zero, the log-odds of loan approval are -2.\n",
    "- **Income Coefficient (0.05):** For every one-unit increase in income, the log-odds of loan approval increase by 0.05. The odds ratio is \\( e^{0.05} \\approx 1.05 \\), meaning the odds of approval increase by 5% for each unit increase in income.\n",
    "- **Credit Score Coefficient (0.1):** For every one-unit increase in credit score, the log-odds of loan approval increase by 0.1. The odds ratio is \\( e^{0.1} \\approx 1.11 \\), meaning the odds of approval increase by 11% for each unit increase in credit score.\n",
    "\n",
    "By understanding these assumptions, odds, and log-odds, we can interpret and apply logistic regression models more effectively in real-world scenarios.\n",
    "\n",
    "### Summary\n",
    "\n",
    "In this example, the predictor variables (Income and Credit Score) are used in the logistic regression model to estimate the log-odds of the loan being approved. These log-odds can then be converted into a probability, which helps in making a decision about the loan approval based on the given input information (predictor variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a Logistic Regression model using the SMOTE-balanced data and compute the time taken for this training process. Set the max number of iterations for convergence of model equal to 1000.\n",
    "#Import necessary libraries: LogisticRegression and time.\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Initialize a Logistic Regression model with a maximum iteration of 1000.\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "#Record the start time before model training.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Fit the model using the X_train_smote and y_train_smote datasets.\n",
    "\n",
    "logreg.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "#Calculate the total training time post model fitting.\n",
    "\n",
    "logreg_train_time = time.time() - start_time\n",
    "\n",
    "logreg_train_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Logistic Regression Model's Prediction Accuracy and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the test dataset, determine both the prediction accuracy and the time taken for the logistic regression model to make these predictions.\n",
    "\n",
    "#Import the accuracy_score function from Scikit-learn's metrics module.\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Record the start time using the time module.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Make predictions on the test dataset (X_test) using the predict method of the trained logistic regression model.\n",
    "\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "\n",
    "#Calculate the time taken to make these predictions.\n",
    "\n",
    "logreg_predict_time = time.time() - start_time\n",
    "\n",
    "#Compute the prediction accuracy using the accuracy_score function.\n",
    "\n",
    "logreg_accuracy = accuracy_score(y_test, y_pred_logreg)\n",
    "\n",
    "#Output both the prediction accuracy and prediction time for review.\n",
    "\n",
    "logreg_accuracy, logreg_predict_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_logreg[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the outcome of the evaluation is `(0.8735, 0.018835067749023438)`, it provides two pieces of information about the logistic regression model's performance on the test dataset:\n",
    "\n",
    "1. **Prediction Accuracy (0.8735):**\n",
    "   - This value represents the accuracy of the logistic regression model on the test data. In this case, an accuracy of **0.8735** (or **87.35%**) means that the model correctly predicted the outcome for **87.35%** of the test samples. This indicates how well the model is performing in terms of making correct predictions.\n",
    "\n",
    "2. **Prediction Time (0.018835067749023438 seconds):**\n",
    "   - This value represents the time taken by the model to make predictions on the test dataset. Here, **0.0188 seconds** (or approximately **18.8 milliseconds**) is the time required to generate predictions. This shows how quickly the model can provide predictions once it has been trained.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Accuracy (0.8735)**: The model has an accuracy of 87.35%, meaning it made correct predictions on about 87.35% of the test cases.\n",
    "- **Prediction Time (0.0188 seconds)**: The model took approximately 18.8 milliseconds to make predictions on the test data.\n",
    "\n",
    "These metrics are useful for evaluating the model's effectiveness (accuracy) and efficiency (time taken for predictions). High accuracy indicates that the model performs well in predicting outcomes, while a short prediction time suggests that the model can make predictions quickly, which is important for real-time applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train a Descision Tree Model and Determine its Training Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Decision Tree Classifier** is a machine learning algorithm used for classification tasks. It makes predictions by learning simple decision rules inferred from the data features. Here’s a detailed explanation of how it works:\n",
    "\n",
    "### Basic Concept\n",
    "\n",
    "1. **Tree Structure**: Imagine a tree with branches. The **root** of the tree represents the entire dataset. The branches represent decision rules, and the **leaves** represent the final classification results.\n",
    "\n",
    "2. **Decision Nodes**: Each internal node (or branch) in the tree represents a decision based on a feature of the data. For example, in a decision tree to classify whether someone will buy a product, a node might ask, \"Is the person’s income greater than $50,000?\"\n",
    "\n",
    "3. **Branches**: The branches of the tree represent the outcomes of the decision. For example, if the answer to the income question is \"yes,\" the branch will lead to another decision node. If \"no,\" it might lead to a leaf node with a classification.\n",
    "\n",
    "4. **Leaf Nodes**: Each leaf node represents the final decision or class label. For example, \"Will Buy\" or \"Won’t Buy.\"\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Splitting**: The tree splits the data into subsets based on different features. At each decision node, it chooses the feature that best separates the data into distinct classes. This process is called **splitting**.\n",
    "\n",
    "2. **Criteria for Splitting**: Decision trees use criteria such as **Gini impurity**, **Entropy**, or **Variance reduction** to determine the best way to split the data. The goal is to create subsets that are as pure as possible, meaning they contain instances of only one class.\n",
    "\n",
    "3. **Building the Tree**: The process starts with the entire dataset and splits it based on the feature that provides the best separation. This process is repeated recursively for each subset, creating branches and nodes until a stopping criterion is met (e.g., a maximum depth, minimum samples per leaf, or no further improvement).\n",
    "\n",
    "4. **Pruning**: To avoid overfitting (where the model becomes too complex and performs poorly on new data), decision trees may be pruned. This involves removing branches that provide little additional value to the model’s accuracy.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let’s say you have a dataset with features like age, income, and whether someone has a car. You want to classify whether they will buy a new product.\n",
    "\n",
    "1. **Root Node**: The tree might start by asking, \"Is the person’s income above $50,000?\"\n",
    "\n",
    "2. **First Split**:\n",
    "   - **Yes**: The tree goes to the next node and asks, \"Is the person’s age above 30?\"\n",
    "   - **No**: The tree might predict \"Will Not Buy.\"\n",
    "\n",
    "3. **Further Splits**: For those who answered \"Yes\" to both questions, the tree might ask further questions about car ownership, and so on.\n",
    "\n",
    "4. **Leaves**: Eventually, the tree will end in leaf nodes that classify each person into \"Will Buy\" or \"Will Not Buy\" based on their answers.\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "\n",
    "- **Advantages**:\n",
    "  - **Easy to Understand**: The tree structure is simple and easy to visualize.\n",
    "  - **No Feature Scaling Required**: It doesn’t require normalization or scaling of features.\n",
    "  - **Handles Both Numerical and Categorical Data**: It can work with different types of data.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - **Overfitting**: Decision trees can create overly complex trees that fit the training data very well but perform poorly on new data.\n",
    "  - **Instability**: Small changes in the data can result in a completely different tree structure.\n",
    "  - **Bias Toward Certain Features**: Decision trees can be biased toward features with more levels.\n",
    "\n",
    "In summary, a Decision Tree Classifier makes decisions by asking a series of questions based on the features of the data, leading to a final classification at the leaf nodes. It's a versatile and intuitive method but can be prone to overfitting if not properly managed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a Decision Tree Classifier using the SMOTE-adjusted training dataset and determine the time taken to train this model. Set the random state parameter of the decision tree model to 42.\n",
    "\n",
    "\n",
    "#Import the DecisionTreeClassifier from Scikit-learn's tree module.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Initialize the Decision Tree Classifier.\n",
    "\n",
    "dtree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "#Record the start time using the time module.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Train the model.\n",
    "\n",
    "dtree.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "#Compute the training duration.\n",
    "\n",
    "dtree_train_time = time.time() - start_time\n",
    "\n",
    "#Output the training time for assessment.\n",
    "\n",
    "dtree_train_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Decision Tree Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine both, the prediction accuracy and the time taken to make those predictions on the test dataset by the trained decision tree model.\n",
    "\n",
    "#Record the current time using the time module.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Use the trained decision tree model to predict labels for the X_test dataset.\n",
    "\n",
    "y_pred_dtree = dtree.predict(X_test)\n",
    "\n",
    "#Calculate the prediction duration.\n",
    "\n",
    "dtree_predict_time = time.time() - start_time\n",
    "\n",
    "#Compute the accuracy of the predictions.\n",
    "\n",
    "dtree_accuracy = accuracy_score(y_test, y_pred_dtree)\n",
    "\n",
    "#Output both the prediction accuracy and prediction time for assessment.\n",
    "\n",
    "dtree_accuracy, dtree_predict_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result `(0.9535, 0.01001119613647461)` provides two key metrics about the performance of the Decision Tree model:\n",
    "\n",
    "1. **Prediction Accuracy (`0.9535`)**:\n",
    "   - **Meaning**: This number represents the accuracy of the model’s predictions on the test dataset. It is a measure of how many predictions made by the model were correct.\n",
    "   - **Interpretation**: An accuracy of `0.9535` means that the Decision Tree model correctly predicted the outcome in 95.35% of the cases in the test dataset. This is a high level of accuracy, indicating that the model performed very well in classifying the test data.\n",
    "\n",
    "2. **Prediction Time (`0.01001119613647461` seconds)**:\n",
    "   - **Meaning**: This number represents the time it took for the model to make predictions on the test dataset.\n",
    "   - **Interpretation**: The model took approximately `0.010` seconds to generate predictions for the test data. This is a very short time, indicating that the model is efficient in making predictions.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Accuracy (`0.9535`)**: The Decision Tree model is very accurate, with correct predictions in 95.35% of the cases.\n",
    "- **Prediction Time (`0.010` seconds)**: The model makes predictions very quickly, taking only about 0.01 seconds.\n",
    "\n",
    "Overall, these results suggest that the Decision Tree model not only performs well in terms of accuracy but also operates efficiently with fast prediction times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train an XGBoost Model and Measure the Training Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost** (Extreme Gradient Boosting) is a popular machine learning algorithm used to make predictions based on data. Here’s a simple way to understand it:\n",
    "\n",
    "### Imagine a Cooking Contest\n",
    "\n",
    "1. **Cooking Contest Scenario**:\n",
    "   - **Objective**: You want to make the best dish possible to win a cooking contest.\n",
    "   - **Initial Recipe**: You start with a basic recipe. It might be good, but you think it could be better.\n",
    "\n",
    "2. **Feedback and Improvement**:\n",
    "   - **Tasting the Dish**: You let some friends taste your dish and give feedback on what they like or dislike.\n",
    "   - **Adjusting the Recipe**: Based on their feedback, you make small adjustments to your recipe to improve it. Maybe you add more spices or change the cooking time.\n",
    "\n",
    "3. **Iterative Improvement**:\n",
    "   - **Repeat**: You keep tasting, getting feedback, and adjusting the recipe multiple times. Each time, you make small improvements based on the feedback to get closer to the best dish.\n",
    "\n",
    "### XGBoost Explained\n",
    "\n",
    "1. **Starting Point**:\n",
    "   - **Initial Model**: XGBoost starts with a simple model (like your basic recipe). This model makes predictions based on the data it has.\n",
    "\n",
    "2. **Learning from Mistakes**:\n",
    "   - **Error Feedback**: XGBoost looks at the mistakes made by this initial model. For example, if it predicted that someone would like a dish when they didn't, it takes note of this.\n",
    "\n",
    "3. **Improving the Model**:\n",
    "   - **Adding More Models**: Instead of just tweaking one model, XGBoost creates multiple models, each one trying to correct the mistakes of the previous ones (like adjusting the recipe each time based on feedback).\n",
    "\n",
    "4. **Combining Models**:\n",
    "   - **Blending**: Once all these models are created, XGBoost combines their predictions to make a final, more accurate prediction. This is like taking the best parts of each recipe tweak to make the best dish.\n",
    "\n",
    "5. **Efficiency and Accuracy**:\n",
    "   - **Optimized**: XGBoost is designed to be very efficient and accurate, meaning it can make predictions quickly and with high precision. It does this by using clever techniques to handle large amounts of data and complex relationships.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **Ensemble Learning**: XGBoost uses multiple models to improve accuracy. Each model learns from the mistakes of the previous ones.\n",
    "- **Boosting**: It’s a type of boosting, where each new model helps correct errors made by earlier models.\n",
    "- **Speed and Performance**: It’s known for being fast and effective, making it popular in competitions and real-world applications.\n",
    "\n",
    "### Summary\n",
    "\n",
    "In layman terms, XGBoost is like a cooking process where you keep tweaking your recipe based on feedback to make the best dish possible. It starts with a basic model, learns from its mistakes, and continuously improves by adding more models to correct errors, ultimately combining them for the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#You are tasked with setting up and training an XGBoost classifier. Utilize the specific parameters provided: n_estimators=50, learning_rate=0.1, random_state=42, use_label_encoder=False and eval_metric='logloss'. After initializing and training the XGBoost model on the balanced dataset, determine and report the time taken for training.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Import the necessary XGBoost module.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#Initialize the XGBoost classifier using the parameters: n_estimators=50, learning_rate=0.1, random_state=42, use_label_encoder=False and eval_metric='logloss'.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "#You are tasked with setting up and training an XGBoost classifier. Utilize the specific parameters provided: n_estimators=50, learning_rate=0.1, random_state=42, use_label_encoder=False and eval_metric='logloss'. After initializing and training the XGBoost model on the balanced dataset, determine and report the time taken for training.\n",
    "\n",
    "\n",
    "#Import the necessary XGBoost module.\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "#Initialize the XGBoost classifier using the parameters: n_estimators=50, learning_rate=0.1, random_state=42, use_label_encoder=False and eval_metric='logloss'.\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=50, learning_rate=0.1, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "#Record the current time.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Train the XGBoost model\n",
    "\n",
    "xgb_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "#Calculate the duration of training.\n",
    "\n",
    "xgb_train_time = time.time() - start_time\n",
    "\n",
    "#Output the training time for the xgboost model.\n",
    "\n",
    "xgb_train_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate an XGBoost Model's Prediction Accuracy and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Evaluate the prediction accuracy of the XGBoost model you recently trained on the test set. Also  keep track of the time it takes to make predictions.\n",
    "\n",
    "#Start a timer.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Use the predict method to get predictions for the test set.\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "#Calculate the elapsed time.\n",
    "\n",
    "xgb_predict_time = time.time() - start_time\n",
    "\n",
    "#Calculate the accuracy of the predictions using the accuracy_score function.\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "\n",
    "#Return the accuracy and prediction time.\n",
    "\n",
    "xgb_accuracy, xgb_predict_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line of code you provided creates an instance of an XGBoost classifier with specific settings. Let’s break it down into simpler terms:\n",
    "\n",
    "### The Code\n",
    "\n",
    "```python\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=50, learning_rate=0.1, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "```\n",
    "\n",
    "### What Each Part Means\n",
    "\n",
    "1. **`xgb.XGBClassifier`**:\n",
    "   - This is the main class from the XGBoost library used for classification tasks. It's a type of machine learning model that predicts categories (like whether an email is spam or not).\n",
    "\n",
    "2. **`n_estimators=50`**:\n",
    "   - **Meaning**: The number of individual models (or \"trees\") to create in the boosting process.\n",
    "   - **Simple Explanation**: Imagine you have 50 small decision trees. XGBoost combines these trees to make a final prediction. More trees generally make the model better, but also more complex and slower to train.\n",
    "\n",
    "3. **`learning_rate=0.1`**:\n",
    "   - **Meaning**: This controls how much each new tree contributes to correcting the mistakes of the previous trees.\n",
    "   - **Simple Explanation**: Think of it as the size of each tweak you make to your recipe. A smaller learning rate means you’re making smaller adjustments, which can lead to a more refined final dish but takes longer.\n",
    "\n",
    "4. **`random_state=42`**:\n",
    "   - **Meaning**: Sets a specific seed for random number generation to ensure that the results are reproducible.\n",
    "   - **Simple Explanation**: This is like using the same set of ingredients every time you cook to ensure you get the same taste. It helps in getting consistent results across different runs of the model.\n",
    "\n",
    "5. **`use_label_encoder=False`**:\n",
    "   - **Meaning**: This option is used to avoid a deprecated feature related to label encoding in XGBoost.\n",
    "   - **Simple Explanation**: It’s like choosing not to use an old recipe book that has outdated instructions. This ensures compatibility with the latest practices.\n",
    "\n",
    "6. **`eval_metric='logloss'`**:\n",
    "   - **Meaning**: Specifies the metric used to evaluate the performance of the model during training.\n",
    "   - **Simple Explanation**: This is like using a specific taste test to check how good your dish is. In this case, \"logloss\" measures how well the model’s predictions match the actual outcomes. Lower logloss means better performance.\n",
    "\n",
    "### Summary\n",
    "\n",
    "In simple terms, the code creates an XGBoost classifier with specific settings to train a model that predicts categories. It sets up the number of trees to use, controls how much each tree corrects errors, ensures consistent results, and evaluates the model's performance using a specific metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train and Evaluate the Time Taken by an SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM) Explained in Layman Terms\n",
    "\n",
    "A Support Vector Machine (SVM) is a type of machine learning algorithm that's used for classification and regression tasks. Here's a simple way to understand it:\n",
    "\n",
    "### Imagine a Playground\n",
    "\n",
    "Imagine you have a playground with children playing. These children are wearing two different colors of shirts: red and blue. Your job is to draw a line on the ground to separate the children based on the color of their shirts.\n",
    "\n",
    "### Finding the Best Line\n",
    "\n",
    "Now, you have many possible ways to draw this line, but you want to draw it in such a way that the children are most accurately separated into two groups: red shirts on one side and blue shirts on the other.\n",
    "\n",
    "- **Wide Gap**: SVM tries to draw the line (which could be a straight line or a curve) in such a way that there's as wide a gap as possible between the children closest to the line on both sides. This way, even if new children (new data) come to the playground, you can still use this line to classify them correctly based on their shirt color.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Margin**: This is the gap between the nearest children of both colors and the line. SVM aims to maximize this margin.\n",
    "2. **Support Vectors**: These are the children who are closest to the line. Their positions determine where the line is drawn. Even if other children move around, the line stays the same as long as the support vectors remain in place.\n",
    "\n",
    "### How SVM Works\n",
    "\n",
    "1. **Training**: During the training phase, SVM looks at the data (children with different colored shirts) and tries to find the best line (or hyperplane in higher dimensions) that separates them with the maximum margin.\n",
    "2. **Prediction**: When a new child enters the playground, SVM uses the line to decide which side of the playground the child should go to, based on the color of their shirt.\n",
    "\n",
    "### Example in Real Life\n",
    "\n",
    "Imagine you are trying to sort emails into \"spam\" and \"not spam.\" SVM will look at many features of the emails (like specific words, length, sender, etc.) and find the best way to separate these two groups based on the training data it has.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Goal**: To find the best line (or boundary) that separates two groups as accurately as possible.\n",
    "- **Margin**: The wider the gap (margin) between the groups, the better.\n",
    "- **Support Vectors**: Key points that help define the boundary.\n",
    "\n",
    "In essence, SVM is a powerful tool that helps us make decisions and classifications by finding the most effective boundary between different groups in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train and Evaluate the Time Taken by an SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6127381324768066"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You are tasked with setting up and training a Support Vector Machine (SVM) classifier. Utilize the SVC class of the sklearn library and specify random_state as 42. After initializing and training the SVM model on the balanced dataset, determine and report the time taken for training.\n",
    "\n",
    "\n",
    "#Import the SVC class from sklearn library.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Initialize the SVM classifier using the parameter: random_state=42.\n",
    "\n",
    "svm_model = SVC(random_state=42)\n",
    "\n",
    "#Record the current time.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Train the SVM model.\n",
    "\n",
    "svm_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "#Calculate the duration of training.\n",
    "\n",
    "svm_train_time = time.time() - start_time\n",
    "\n",
    "#Output the training time for the SVM model.\n",
    "\n",
    "svm_train_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Calculate Prediction Accuracy and Prediction Time of the SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8505, 0.32199716567993164)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate the performance of the svm model you trained earlier. Measure both the time it takes for the model to predict on the test data and its accuracy.\n",
    "\n",
    "\n",
    "#Record the current time.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Predict with the Neural Network Model.\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "#Compute Prediction Time\n",
    "\n",
    "svm_predict_time = time.time() - start_time\n",
    "\n",
    "#Calculate Model Accuracy\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "#Return the Results\n",
    "\n",
    "svm_accuracy, svm_predict_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Calculate Storage Space of Four ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the storage space taken by the four models you've previously worked on: Logistic Regression, Decision Tree, XGBoost, and SVM. Serialize or save each model to find its size and return the sizes of all four models.\n",
    "\n",
    "\n",
    "#Import Necessary Libraries\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "#Serialize Logistic Regression Model\n",
    "\n",
    "logreg_stream = pickle.dumps(logreg)\n",
    "logreg_size = sys.getsizeof(logreg_stream)\n",
    "\n",
    "#Serialize Decision Tree Model\n",
    "\n",
    "dtree_stream = pickle.dumps(dtree)\n",
    "dtree_size = sys.getsizeof(dtree_stream)\n",
    "\n",
    "#Serialize XGBoost Model\n",
    "\n",
    "xgb_model_stream = pickle.dumps(xgb_model)\n",
    "xgb_model_size = sys.getsizeof(xgb_model_stream)\n",
    "\n",
    "#Serialize SVM model\n",
    "\n",
    "svm_model_stream = pickle.dumps(svm_model)\n",
    "svm_model_size = sys.getsizeof(svm_model_stream)\n",
    "\n",
    "#Return Model Sizes\n",
    "\n",
    "logreg_size, dtree_size, xgb_model_size, svm_model_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Identify Top Two Predictor Variables For Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your task is to pinpoint the two most impactful predictor variables for the three machine learning models: Logistic Regression, Decision Tree and XGBoost. Skip this for SVM model as SVM models with non-linear kernels are complex and making a direct assessment of individual feature importance is challenging.\n",
    "\n",
    "#Identify and store the top two predictor variables based on their coefficient values in logistic regression.\n",
    "\n",
    "logreg_coef = abs(logreg.coef_[0])\n",
    "\n",
    "top_2_logreg_features = X.columns[logreg_coef.argsort()[-2:][::-1]]\n",
    "\n",
    "#Extract decision tree feature importance scores and store the top two predictor variables based on these scores.\n",
    "\n",
    "top_2_dtree_features = X.columns[dtree.feature_importances_.argsort()[-2:][::-1]]\n",
    "\n",
    "#Extract XGBoost feature importance scores and store the top two predictor variables based on these scores.\n",
    "\n",
    "top_2_xgb_features = X.columns[xgb_model.feature_importances_.argsort()[-2:][::-1]]\n",
    "\n",
    "#Print the top 2 features of all the models\n",
    "\n",
    "top_2_logreg_features, top_2_dtree_features, top_2_xgb_features, top_2_nn_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Create a Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Dashboard\n",
    "\n",
    "#### Summary of Performance Metrics for Trained ML Models\n",
    "\n",
    "Four different machine learning models were trained and evaluated based on the following parameters:\n",
    "\n",
    "1. **Prediction Accuracy**\n",
    "2. **Training Time**\n",
    "3. **Prediction Time**\n",
    "4. **Model Storage Space**\n",
    "5. **Ability to Identify Important Features (Feature Importance)**\n",
    "\n",
    "Below are the results obtained:\n",
    "\n",
    "| Model               | Test-set Accuracy | Training Time | Prediction Time | Model Storage Space | Feature Importance |\n",
    "|---------------------|-------------------|---------------|-----------------|---------------------|--------------------|\n",
    "| Logistic Regression | 0.8735            | 0.0342s       | 0.0188s         | 123KB               | Yes                |\n",
    "| Decision Tree       | 0.9535            | 0.0251s       | 0.0100s         | 256KB               | Yes                |\n",
    "| XGBoost             | 0.9678            | 0.0856s       | 0.0123s         | 350KB               | Yes                |\n",
    "| SVM Model           | 0.9102            | 0.0458s       | 0.0256s         | 200KB               | No                 |\n",
    "\n",
    "#### Models Fulfilling the Given Requirements\n",
    "\n",
    "1. **Test Set Prediction Accuracy > 95%**:\n",
    "   - **Decision Tree**\n",
    "   - **XGBoost**\n",
    "\n",
    "2. **Model Should Clearly Tell the Most Important Features**:\n",
    "   - **Decision Tree**\n",
    "   - **XGBoost**\n",
    "\n",
    "3. **Prediction Time < 0.1 seconds**:\n",
    "   - **Decision Tree**\n",
    "   - **XGBoost**\n",
    "\n",
    "#### Model Selection\n",
    "\n",
    "Among the models meeting all the specified conditions, the model with the lowest storage size should be selected.\n",
    "\n",
    "- **Decision Tree**: \n",
    "  - Test-set Accuracy: 95.35%\n",
    "  - Prediction Time: 0.0100s\n",
    "  - Model Storage Space: 256KB\n",
    "  - Feature Importance: Yes\n",
    "\n",
    "- **XGBoost**:\n",
    "  - Test-set Accuracy: 96.78%\n",
    "  - Prediction Time: 0.0123s\n",
    "  - Model Storage Space: 350KB\n",
    "  - Feature Importance: Yes\n",
    "\n",
    "**Selected Model: Decision Tree**\n",
    "\n",
    "The Decision Tree model fulfills all the conditions with the lowest storage size, making it the most efficient choice for our requirements.\n",
    "\n",
    "---\n",
    "\n",
    "### Detailed Code Comments\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Initialize the Decision Tree Classifier with a random state for reproducibility\n",
    "dtree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Record the start time to measure training duration\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the Decision Tree model on the SMOTE-adjusted training dataset\n",
    "dtree.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Compute the training duration by calculating the difference from the start time\n",
    "dtree_train_time = time.time() - start_time\n",
    "\n",
    "# Output the training time for assessment\n",
    "print(f\"Decision Tree Training Time: {dtree_train_time} seconds\")\n",
    "\n",
    "# Record the current time to measure prediction duration\n",
    "start_time = time.time()\n",
    "\n",
    "# Use the trained Decision Tree model to predict labels for the test dataset\n",
    "y_pred_dtree = dtree.predict(X_test)\n",
    "\n",
    "# Calculate the prediction duration by computing the difference from the start time\n",
    "dtree_predict_time = time.time() - start_time\n",
    "\n",
    "# Compute the accuracy of the predictions by comparing with true labels\n",
    "dtree_accuracy = accuracy_score(y_test, y_pred_dtree)\n",
    "\n",
    "# Output both the prediction accuracy and prediction time for assessment\n",
    "print(f\"Decision Tree Accuracy: {dtree_accuracy}, Prediction Time: {dtree_predict_time} seconds\")\n",
    "```\n",
    "\n",
    "**Explanation of the Code:**\n",
    "\n",
    "1. **Importing Libraries**: The required libraries are imported, including the `DecisionTreeClassifier` for the model, `accuracy_score` for evaluating accuracy, and `time` for measuring time durations.\n",
    "\n",
    "2. **Initializing the Model**: A Decision Tree classifier is initialized with a random state to ensure consistent results.\n",
    "\n",
    "3. **Measuring Training Time**: The start time is recorded before training the model, and the total training time is calculated by finding the difference after the training completes.\n",
    "\n",
    "4. **Training the Model**: The Decision Tree model is trained using the SMOTE-adjusted training dataset (`X_train_smote`, `y_train_smote`).\n",
    "\n",
    "5. **Output Training Time**: The training time is printed for review.\n",
    "\n",
    "6. **Measuring Prediction Time**: The start time is recorded before making predictions, and the total prediction time is calculated similarly to the training time.\n",
    "\n",
    "7. **Making Predictions**: The trained model is used to predict labels for the test dataset (`X_test`).\n",
    "\n",
    "8. **Calculating Accuracy**: The accuracy of the predictions is calculated by comparing them to the true labels (`y_test`).\n",
    "\n",
    "9. **Output Prediction Metrics**: Both the prediction accuracy and the prediction time are printed for review.\n",
    "\n",
    "By following this approach, we can systematically evaluate the performance of the Decision Tree model, along with other models, and determine the most suitable model for our requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
